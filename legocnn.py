# -*- coding: utf-8 -*-
"""LegoCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C7bVOv55_auuCWDkHCyBKfHWaxbVSuuJ

# Image Classification Task

On this notebook we will build and train a CNN in order to make a in Image Classification task on a dataset composed by 6379 images of lego bricks divided in 16 classes, that represents different categories and measures of these magic bricks.

## Importing libraries
"""

import pandas as pd
import numpy as np
import os
import seaborn as sns
import torch
import torchvision.transforms as T
import torch.nn as nn
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from PIL import Image
from torch.utils.data import random_split
from torchvision.utils import make_grid
from tqdm.notebook import tqdm
from sklearn.metrics import confusion_matrix

"""## Importing Dataset

With these pieces of code, we are importing our dataset from our own personal Google Drive folder.
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/LegoBricksDataset.zip

dest_dir= "/content/LEGO brick images v1"

classes = os.listdir(dest_dir)

print(classes)

sizedf = {}

for category in classes:
  sizedf[category] = len(os.listdir(dest_dir + '/' + category))

sizedf

class LegoDataset(Dataset):
  def __init__(self, dset_dir, transforms=T.Compose([])):
    self.dset_dir = Path(dset_dir)
    self.transforms = transforms
    self.files = []

    folders = sorted(os.listdir(self.dset_dir))
    for folder in folders:
      class_idx = folders.index(folder)
      folder_dir = self.dset_dir/folder
      files = os.listdir(folder_dir)
      self.files += [{"file": folder_dir/x, "class": class_idx} for x in files]

  def __len__(self):
    return len(self.files)

  def __getitem__(self, i):
    item = self.files[i]
    file = item['file']
    class_idx = torch.tensor(item['class'])

    img = Image.open(file).convert("RGB")
    img = self.transforms(img)
    return img, class_idx

transforms = T.Compose([
        T.Resize(32),
        T.ToTensor(),
        T.Normalize(0.5, 0.5)
    ])

dset = LegoDataset(dest_dir, transforms=transforms)

print(len(dset))

"""Our dataset contains 6379 images of 16 classes.

## Splitting

Now we are going to split our data into train, validation and test set.
"""

val_size = len(dset)//10
test_size = len(dset)//5
train_size = len(dset) - val_size -test_size
train_dset, val_dset, test_dset = random_split(dset, [train_size, val_size, test_size])
len(train_dset), len(val_dset),len(test_dset)

data, label = train_dset[0]
print(data.shape)

train_loader = DataLoader(train_dset, batch_size=32, shuffle=True, drop_last=True, num_workers=2)
test_loader = DataLoader(test_dset, batch_size=32, shuffle=False, drop_last=False, num_workers=2)
val_loader = DataLoader(val_dset, batch_size=32, shuffle=False, drop_last=False, num_workers=2)

def show_batch(train_loader):
    for images, _ in train_loader:
        fig, ax = plt.subplots(figsize=(12,12))
        ax.set_xticks([]); ax.set_yticks([])
        ax.imshow(make_grid(images[:32], nrow=8).permute(1,2,0))
        break
        
show_batch(train_loader)

inputs, labels = next(iter(train_loader))
print(inputs.shape)
print(labels.shape)

"""## Creating our CNN

The minimal layers we need for defining a CNN are:



*   Convolutional layer:

  `nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation)`

*   Non-linear layer:

  `nn.ReLU()`

*   Max pooling:

  `nn.MaxPool2d(kernel_size, stride)`

### 5 layers

First, we are going to define only the 5 layers in order to find the output that will be the input of the dense layer.
"""

class FirstCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8),
      # Layer 2
      nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(16),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 3
      nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(32),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 4
      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(64),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 5
      nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(128),
      nn.AdaptiveMaxPool2d(output_size=2)
    )

  # Forward
  def forward(self, x):
    output = self.layers(x)
    return output

net = FirstCNN()
# Get an element from the dataset

"""Now we are assigning an image to a variable in order to test the model and find the output size."""

test_x, _ = train_dset[0] # each element of the dataset is a couple (image, label)

test_x.size()

test_x = test_x.unsqueeze(dim=0)
test_x.size()

output = net(test_x)
output.shape

out_features = output.size(1)*output.size(2)*output.size(3)
print(out_features)

"""Our output size is 512."""

class FirstCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8),
      # Layer 2
      nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(16),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 3
      nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(32),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 4
      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(64),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 5
      nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(128),
      nn.AdaptiveMaxPool2d(output_size=2)
    )
    self.fc_layers = nn.Sequential(
        nn.Linear(512, 1024), 
        nn.ReLU(),
        nn.Linear(1024, 16)
    )
  # Forward
  def forward(self, x):
    x = self.layers(x)
    x = x.view(x.size(0), -1)
    output = self.fc_layers(x)
    return output

"""Now we are going to define the training algorithm."""

def train(net, loaders, optimizer, criterion, epochs=100, device=torch.device('cpu')):
    try:
        net = net.to(device)
        print(net)
        # Initialize history
        history_loss = {"train": [], "val": [], "test": []}
        history_accuracy = {"train": [], "val": [], "test": []}
        results = [0,0,0]
        valiloss = None
        # Process each epoch
        for epoch in range(epochs):
            # Initialize epoch variables
            sum_loss = {"train": 0, "val": 0, "test": 0}
            sum_accuracy = {"train": 0, "val": 0, "test": 0}
            # Process each split
            for split in ["train", "val", "test"]:
                if split == "train":
                  net.train()
                else:
                  net.eval()
                # Process each batch
                for (input, labels) in tqdm(loaders[split],desc=split):
                    # Move to CUDA
                    input = input.to(device)
                    labels = labels.to(device)
                    # Reset gradients
                    optimizer.zero_grad()
                    # Compute output
                    pred = net(input)
                    loss = criterion(pred, labels)
                    # Update loss
                    sum_loss[split] += loss.item()
                    # Check parameter update
                    if split == "train":
                        # Compute gradients
                        loss.backward()
                        # Optimize
                        optimizer.step()
                    # Compute accuracy
                    _,pred_labels = pred.max(1)
                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)
                    # Update accuracy
                    sum_accuracy[split] += batch_accuracy
            # Compute epoch loss/accuracy
            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in ["train", "val", "test"]}
            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in ["train", "val", "test"]}
            # Update history
            for split in ["train", "val", "test"]:
                history_loss[split].append(epoch_loss[split])
                history_accuracy[split].append(epoch_accuracy[split])
            # Print info
            print(f"Epoch {epoch+1}:",
                  f"TrL={epoch_loss['train']:.4f},",
                  f"TrA={epoch_accuracy['train']:.4f},",
                  f"VL={epoch_loss['val']:.4f},",
                  f"VA={epoch_accuracy['val']:.4f},",
                  f"TeL={epoch_loss['test']:.4f},",
                  f"TeA={epoch_accuracy['test']:.4f},")
            if valiloss is None or epoch_loss['val'] < valiloss:
              valiloss = epoch_loss['val']
              results = [epoch_accuracy['train'], epoch_accuracy['val'], epoch_accuracy['test']]  

    except KeyboardInterrupt:
        print("Interrupted")
    finally:
        # Plot loss
        plt.title("Loss")
        for split in ["train", "val", "test"]:
            plt.plot(history_loss[split], label=split)
        plt.legend()
        plt.show()
        # Plot accuracy
        plt.title("Accuracy")
        for split in ["train", "val", "test"]:
            plt.plot(history_accuracy[split], label=split)
        plt.legend()
        plt.show()
        return results

"""Before training, we would like to know if the GPU is available. If not, we are going to use CPU, but it's preferrable to use cuda, because it improves performances in a huge way."""

torch.cuda.is_available()

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

net1 = FirstCNN()
optimizer = optim.SGD(net1.parameters(), lr = 0.005)
criterion = nn.CrossEntropyLoss()


# Define dictionary of loaders
loaders = {"train": train_loader,
           "val": val_loader,
           "test": test_loader}

results_1 = train(net1, loaders, optimizer, criterion, epochs=20, device=device)

print(results_1)

"""### 4 Layers

Now we are going to delete the level 5 from the previous model.
"""

class SecondCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8),
      # Layer 2
      nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(16),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 3
      nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(32),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 4
      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(64),
      nn.MaxPool2d(kernel_size=2, stride=2)
    )

  # Forward
  def forward(self, x):
    output = self.layers(x)
    return output

net = SecondCNN()

# Get an element from the dataset
test_x, _ = train_dset[0] # each element of the dataset is a couple (image, label)

test_x.size()

test_x = test_x.unsqueeze(dim=0)
test_x.size()

output = net(test_x)
output.shape

out_features = output.size(1)*output.size(2)*output.size(3)
print(out_features)

class SecondCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8),
      # Layer 2
      nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(16),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 3
      nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(32),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 4
      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(64),
      nn.MaxPool2d(kernel_size=2, stride=2)
    )
    self.fc_layers = nn.Sequential(
        nn.Linear(576, 1024), 
        nn.ReLU(),
        nn.Linear(1024, 16)
    )
  # Forward
  def forward(self, x):
    x = self.layers(x)
    x = x.view(x.size(0), -1)
    output = self.fc_layers(x)
    return output

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

net2 = SecondCNN()
optimizer = optim.SGD(net2.parameters(), lr = 0.005)
criterion = nn.CrossEntropyLoss()


# Define dictionary of loaders
loaders = {"train": train_loader,
           "val": val_loader,
           "test": test_loader}

results_2 = train(net2, loaders, optimizer, criterion, epochs=20, device=device)

print(results_2)

"""## 3 layers """

class ThirdCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8),
      # Layer 2
      nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(16),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 3
      nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(32),
      nn.MaxPool2d(kernel_size=2, stride=2)
    )

  # Forward
  def forward(self, x):
    output = self.layers(x)
    return output

net = ThirdCNN()

# Get an element from the dataset
test_x, _ = train_dset[0] # each element of the dataset is a couple (image, label)

test_x.size()

test_x = test_x.unsqueeze(dim=0)
test_x.size()

output = net(test_x)
output.shape

out_features = output.size(1)*output.size(2)*output.size(3)
print(out_features)

class ThirdCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8),
      # Layer 2
      nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(16),
      nn.MaxPool2d(kernel_size=2, stride=2),
      # Layer 3
      nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(32),
      nn.MaxPool2d(kernel_size=2, stride=2)
    )
    self.fc_layers = nn.Sequential(
        nn.Linear(1568, 1024), 
        nn.ReLU(),
        nn.Linear(1024, 16)
    )
  # Forward
  def forward(self, x):
    x = self.layers(x)
    x = x.view(x.size(0), -1)
    output = self.fc_layers(x)
    return output

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

net3 = ThirdCNN()
optimizer = optim.SGD(net3.parameters(), lr = 0.005)
criterion = nn.CrossEntropyLoss()


# Define dictionary of loaders
loaders = {"train": train_loader,
           "val": val_loader,
           "test": test_loader}

results_3 = train(net3, loaders, optimizer, criterion, epochs=20, device=device)

print(results_3)

"""## 2 layers """

class FourthCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8),
      # Layer 2
      nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(16),
      nn.MaxPool2d(kernel_size=2, stride=2)
    )

  # Forward
  def forward(self, x):
    output = self.layers(x)
    return output

net = FourthCNN()

# Get an element from the dataset
test_x, _ = train_dset[0] # each element of the dataset is a couple (image, label)

test_x.size()

test_x = test_x.unsqueeze(dim=0)
test_x.size()

output = net(test_x)
output.shape

out_features = output.size(1)*output.size(2)*output.size(3)
print(out_features)

class FourthCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8),
      # Layer 2
      nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(16),
      nn.MaxPool2d(kernel_size=2, stride=2)
    )
    self.fc_layers = nn.Sequential(
        nn.Linear(3600, 1024), 
        nn.ReLU(),
        nn.Linear(1024, 16)
    )
  # Forward
  def forward(self, x):
    x = self.layers(x)
    x = x.view(x.size(0), -1)
    output = self.fc_layers(x)
    return output

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

net4 = FourthCNN()
optimizer = optim.SGD(net4.parameters(), lr = 0.005)
criterion = nn.CrossEntropyLoss()


# Define dictionary of loaders
loaders = {"train": train_loader,
           "val": val_loader,
           "test": test_loader}

results_4 = train(net4, loaders, optimizer, criterion, epochs=20, device=device)

print(results_4)

"""## 1 layer """

class FifthCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8)
    )

  # Forward
  def forward(self, x):
    output = self.layers(x)
    return output

net = FifthCNN()

# Get an element from the dataset
test_x, _ = train_dset[0] # each element of the dataset is a couple (image, label)

test_x.size()

test_x = test_x.unsqueeze(dim=0)
test_x.size()

output = net(test_x)
output.shape

out_features = output.size(1)*output.size(2)*output.size(3)
print(out_features)

class FifthCNN(nn.Module):

  #Constructor
  def __init__(self, in_size=3, out_size=16, use_norm=False):
    # Call parent contructor
    super().__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=in_size, out_channels=8, kernel_size=3, padding=0, stride=1),
      nn.ReLU(),
      nn.BatchNorm2d(8)
    )
    self.fc_layers = nn.Sequential(
        nn.Linear(7200, 1024), 
        nn.ReLU(),
        nn.Linear(1024, 16)
    )
  # Forward
  def forward(self, x):
    x = self.layers(x)
    x = x.view(x.size(0), -1)
    output = self.fc_layers(x)
    return output

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

net5 = FifthCNN()
optimizer = optim.SGD(net5.parameters(), lr = 0.005)
criterion = nn.CrossEntropyLoss()


# Define dictionary of loaders
loaders = {"train": train_loader,
           "val": val_loader,
           "test": test_loader}

results_5 = train(net5, loaders, optimizer, criterion, epochs=20, device=device)

print(results_5)

"""## Model Evaluation

We implemented a few lines of code into the training algorithm in order to collect results for every model. Now we are going to compare them into a dataframe and then we will assign the variable `net` to the best network. It will be evaluated through a confusion matrix to see how accurate is the model in predicting each of the classes.
"""

percentage1 = [str(round(num * 100, 2))+"%" for num in results_1]
percentage2 = [str(round(num * 100, 2))+"%" for num in results_2]
percentage3 = [str(round(num * 100, 2))+"%" for num in results_3]
percentage4 = [str(round(num * 100, 2))+"%" for num in results_4]
percentage5 = [str(round(num * 100, 2))+"%" for num in results_5]

print(percentage1)

results = [percentage1, percentage2, percentage3, percentage4, percentage5]
models = ["Five layers", "Four layers", "Three layers", "Two Layers", "One Layer"]
df = pd.DataFrame(results, columns =['Train', 'Validation', 'Test'], index = models, dtype = str)
df

"""Our best model is the one with two layers. We already know looking at the table, and we could hardcode this, but we prefer to have a code that is able to decide it automatically every time it is ran."""

#Choose the best CNN
resultlist = []
for result in (results_1, results_2, results_3, results_4, results_5):
  resultlist.append(result[2])
print(resultlist)
if max(resultlist) == results_1[2]:
  net = net1
  print("Best Network: 5 layer CNN")
elif max(resultlist) == results_2[2]:
  net = net2
  print("Best Network: 4 layers CNN")
elif max(resultlist) == results_3[2]:
  net = net3
  print("Best Network: 3 layers CNN")
elif max(resultlist) == results_4[2]:
  net = net4
  print("Best Network: 2 layers CNN")
elif max(resultlist) == results_5[2]:
  net = net5
  print("Best Network: 1 layers CNN")

"""## Confusion Matrix

We've seen which is the best model, we've seen how accurate it is in doing prediction to the test dataset, now we will evaluate how this accuracy is divided for each class. We are taking in consideration the fact that the model could perfectly predict some variables and perform bad in other ones.
"""

# Initialize lists
y_pred = []
y_true = []

# Obtain predictions and true labels
for inputs, labels in test_loader:
  inputs, labels = inputs.cuda(), labels.cuda()
  output = net(inputs) # Feed Network
  output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()
  y_pred.extend(output) # Save Prediction
  
  labels = labels.data.cpu().numpy()
  y_true.extend(labels) # Save True labels

# Build and print the confusion matrix
cm = confusion_matrix(y_true, y_pred)
cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(17,10))
sns.heatmap(cmn, annot=True, xticklabels=classes, yticklabels=classes)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show(block=False)